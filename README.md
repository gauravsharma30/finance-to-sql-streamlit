## Experiment Results
### Stage 1
#### 1a_tougher
| S.No | Matching Input       | Model Used           | Accuracy | F1 Score | Remarks |
|------|----------------------|----------------------|----------|----------|---------|
| 1    | Terms                | all-mpnet-base-v2    | 0.29     | 0.23     |         |
| 2    | Desc                 | all-mpnet-base-v2    | 0.35     | 0.252    |         |
| 3    | Terms + Desc         | all-mpnet-base-v2    | 0.36     | 0.29     |         |
| 4    | Terms + Desc         | bge-base-en-v1.5     | 0.31     | 0.23     |         |
| 5    | Terms + Desc         | bge-large-en-v1.5    | 0.36     | 0.29     |         |
| 6    | Terms + Desc         | mxbai-embed-large-v1 | 0.38     | 0.32     |         |
| 7    | Terms + Desc         | bge-large-en-v1.5    | 0.39     | 0.325    | added 'defined as' as stitching term |
| 8    | Terms + Desc         | all-mpnet-base-v2    | 0.39     | 0.318    | added 'is' as stitching term |
| 9    | Terms + Desc         | bge-large-en-v1.5    | 0.41     | 0.341    | added 'can be defined as' as stitching term |
| 10   | Terms + Desc         | bge-large-en-v1.5    | 0.41     | 0.341    | added re-ranker: didn't help |
| 11   | Terms + Desc         | bge-large-en-v1.5    | 0.46     | 0.355    | miniLM re-ranker finetuned on hard-neg, noisy queries |
| 12   | Terms + Desc         | bge-large-en-v1.5    | **0.49**     | 0.40     | sno 11 + reranker trained with glossary desc |
| 13   | Terms + Desc         | bge-large-en-v1.5    | 0.46     | 0.34     | sno 12 + reranker trained with simpler data |


#### 1a_simpler
| S.No | Matching Input       | Model Used           | Accuracy | F1 Score | Remarks |
|------|----------------------|----------------------|----------|----------|---------|
| 1    | Terms + Desc         | bge-large-en-v1.5    | 0.863    | 0.845    |         |
| 2    | Terms + Desc         | bge-large-en-v1.5    | 0.954    | 0.941    | With re-ranker, top5 |
| 3    | Terms + Desc         | bge-large-en-v1.5    | 0.962    | 0.95     | With re-ranker, top10 |

#### 1a_simpler_noisy  
- This contains natural spelling, grammatical mistakes

| S.No | Matching Input       | Model Used           | Accuracy | F1 Score | Remarks |
|------|----------------------|----------------------|----------|----------|---------|
| 1    | Terms + Desc         | bge-large-en-v1.5    | 0.9688   | 0.9583   | With re-ranker, top10 |
| 2    | Terms + Desc         | bge-large-en-v1.5    | 0.89     | 0.86   | miniLM re-ranker finetuned on hard-neg, noisy queries |
| 3    | Terms + Desc         | bge-large-en-v1.5    | **0.87**     | 0.85   | sno 2 + glossary data 4b with desc |
| 4    | Terms + Desc         | bge-large-en-v1.5    | 0.82     | 0.78   | sno 3 + finetuned on simpler data too |


### Stage 2
#### 1b_clean_data
| S.No | Glossary Input       | Model Used           | Accuracy | F1 Score | Remarks |
|------|----------------------|----------------------|----------|----------|---------|
| 1    | Term                 | all-mpnet-base-v2    | 0.41     | 0.26     |         |
| 2    | Term                 | bge-large-en-v1.5    | 0.48     | 0.32     |         |
| 3    | Term + Desc          | bge-large-en-v1.5    | 0.44     | 0.29     |         |
| 4    | Term                 | bge-large-en-v1.5    | 0.38     | 0.23     | With reranker |
| 5    | Term                 | bge-large-en-v1.5    | 0.51     | 0.35     | With reranker fine-tuned** |
| 6    | Term                 | bge-large-en-v1.5    | 0.55     | 0.38     | reranker miniLM fine-tuned + weights b/w ranker & sim |
| 7    | Term                 | bge-large-en-v1.5    | 0.62     | 0.45     | reranker miniLM fine-tuned on more hard-negative + weights b/w ranker & sim  |

## Ideas to Try
- Improve the re-ranker's data with top-k data -> what is it really getting confused at?
- Instead of binary scoring, use triplet scoring with hard-negative being different from random negative
- Add in type, etc. in the matching queries. 
- Fine-tune BGE with contrastive loss

## Files
nl2glossary.py: 
- natural language to glossary
- i/p, 1: queries_and_glossary.csv (generated by us: query and GT glossary)
- i/p, 2: glossary_v1.csv (given by finalyser: glossary and description)
- o/p: nl2glossary.csv (storing csv for gt, pred)

glossary2grouping.py
- i/p, 1: nl2glossary.csv (glossary, GT grouping lables & id)
- You can use **glossary2label.csv** as this file contains the glossary, GT grouping_label, GT grouping_id for better testing.
  - As the Glossary in this one has the correct grouping_label.
  - Check the columns' names, as they might be different.
  - For example: Glossary -> glossary, GT grouping_label -> ground_label. 
- i/p, 2: fbi_groping_master.csv (grouping labels, id)
- o/p: glossary2label_results

metric.py
- I/p: Results file from respective step
- O/P: Generate Metrics for given step
<br/>

glossary2label.csv
- Contains the Glossary, ground truth grouping_label, GT grouping_id.
- Columns: glossary, ground_label, grouping_id
<br/>

nl2glossary.csv
- Contains Natural Query, GT Glossary, the predicted Glossary, and the similarity score.
- Columns: NL_Query,	GT_Glossary,	Predicted_Glossary,	Similarity_Score
<br/>

glossary2label_results.csv
- Contains the Glossary, it GT grouping_label, GT grouping_id, Predicted grouping_label, Predicted grouping_id.
- Columns: glossary,	ground_label,	grouping_id,	Predicted_Label,	Predicted_Grouping_ID


<br/>



## Quantitative results
- nl2glossary.csv -> Contains Natural Query, Ground Truth Glossary, and the Predicted Glossary.
- glossary2label_results.csv -> Contains Glossary, Ground Truth grouping_label, Ground Truth grouping_id, Predicted grouping_label, Predicted grouping_id
<br/

## Run locally
- Use the app_v2.py file for running the model locally.
- Use the PEM file you have to connect it to the database.

